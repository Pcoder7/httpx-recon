name: Parallel HTTPX Probe

on:
  workflow_dispatch:

permissions:
  contents: write

env:
  
  MAX_PARALLEL: 20         # <— throttle HTTPX jobs

jobs:
  prepare_matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set_matrix.outputs.matrix }}
      unique_domains: ${{ steps.set_matrix.outputs.unique_domains }}
    steps:
      - uses: actions/checkout@v3
        with: 
          fetch-depth: 0 

      - name: Build domain matrix from results/*/all_resolved.txt
        id: set_matrix
        run: |
          # 1. Discover domain folders with all_resolved.txt
          mapfile -t domains < <(
            find results -mindepth 1 -maxdepth 1 -type d \
              -exec test -f "{}/all_resolved.txt" \; -print \
            | sed 's|results/||'
          )
      
          # 2. If no domains are found, emit an empty matrix
          if [ ${#domains[@]} -eq 0 ]; then
            echo "No domains found under results/ — emitting empty matrix."
            echo "matrix<<EOF" >> $GITHUB_OUTPUT
            echo "[]"           >> $GITHUB_OUTPUT
            echo "EOF"          >> $GITHUB_OUTPUT
          else
            pairs=()
      
            for d in "${domains[@]}"; do
              SRC="results/$d/all_resolved.txt"

              # skip if file somehow missing  
              [ -f "$SRC" ] || continue
              
              # ← step A: get the total line count
              LINES=$(wc -l < "$SRC")
              echo "Source $SRC has $LINES lines"
      
              # 3. Create per-domain chunk dir & split into 100-line files
              CHUNK_DIR="results/$d/chunks"
              mkdir -p "$CHUNK_DIR"
              if [ "$LINES" -ge 1000 ]; then
                echo "— ≥1 000 lines: splitting into 30 equal chunks"
                split -n l/30 \
                      --numeric-suffixes=1 --suffix-length=2 --additional-suffix=.txt \
                      "$SRC" "$CHUNK_DIR/chunk_"
              elif [ "$LINES" -gt 200 ]; then
                echo "— > 200 lines: splitting into 20 equal chunks"
                split -n l/10 \
                      --numeric-suffixes=1 --suffix-length=2 --additional-suffix=.txt \
                      "$SRC" "$CHUNK_DIR/chunk_"
              else
                echo "— ≤ 50 lines: splitting into 50-line chunks"
                split -l 50 \
                      --numeric-suffixes=1 --suffix-length=2 --additional-suffix=.txt \
                      "$SRC" "$CHUNK_DIR/chunk_"
              fi          
      
              # 4. Debug: list out what we just created
              echo "Contents of $CHUNK_DIR:"
              ls -R "$CHUNK_DIR" || echo "(none found)"
      
              # 5. Build a JSON pair for each chunk file
              for CH in "$CHUNK_DIR"/chunk_*; do
                pairs+=( "{\"domain\":\"$d\",\"chunk\":\"$CH\"}" )
              done
            done
      
            # 6. Emit the chunk-level matrix
            echo "matrix<<EOF" >> $GITHUB_OUTPUT
            printf '%s\n' "${pairs[@]}" | jq -s . >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          fi
      
          # 7. Emit the unique domain list (unchanged)
          domains_json=$(printf '%s\n' "${domains[@]}" | jq -R . | jq -s .)
          {
            echo "unique_domains<<EOF"
            echo "${domains_json}"
            echo "EOF"
          } >> "$GITHUB_OUTPUT"    
                

      - name: Upload chunks artifact
        uses: actions/upload-artifact@v4
        with:
          name: httpx-chunks
          path: results/*/chunks/**
  
  httpx:
    needs: prepare_matrix
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}    
    if: ${{ needs.prepare_matrix.outputs.matrix != '[]' }}
    strategy:
      fail-fast: false
      matrix:
        pair: ${{ fromJson(needs.prepare_matrix.outputs.matrix) }}
      max-parallel: 20

    steps:
     
     
      - name: Ensure httpx & anew are installed
        run: |
         
          if ! command -v httpx >/dev/null; then
            echo "httpx installed"
                
          else
            echo "httpx already installed"
          fi
          if ! command -v anew >/dev/null; then
            go install -v github.com/tomnomnom/anew@latest
            
          else
            echo "anew already installed"
          fi
                
      - name: Download chunk files
        uses: actions/download-artifact@v4
        with: 
          name: httpx-chunks 
          path: results/
   
  
      - name: Probe with httpx on ${{ matrix.pair.domain }}
        run: |
       
          D="${{ matrix.pair.domain }}"
          CH="${{ matrix.pair.chunk }}"
          OUT="results/${D}/httpx_out"
          mkdir -p "$OUT"
      
          LINES=$(wc -l < "$CH")
          echo "▶ Processing $CH — $LINES lines"

          if [ "$LINES" -le 50 ]; then
            # small: single httpx run
            echo "— ≤1500 lines: single httpx with 100 threads"
            #httpx -l "$CH" -silent -threads 100 \
            #  -o "${OUT}/httpx_$(basename "$CH").txt"
            nuclei -l "$CH" -silent -c 10 -probe-concurrency 100 -t ~/nuclei-templates/http/exposed-panels/ -bs 10 -scan-strategy template-spray -rl 50 -o -o "${OUT}/httpx_$(basename "$CH").txt"

          else
            # big: split into 1500-line sub-files
            TMP=$(mktemp -d)
            # suffix-length=2 covers up to 99 subchunks (we’ll have ≤30)
            split -l 1500 --numeric-suffixes=1 --suffix-length=2 \
                  --additional-suffix=.txt \
                  "$CH" "$TMP/sub_"

            # decide parallelism & threads based on original chunk size
            if [ "$LINES" -gt 300 ]; then
              JOBS=8; #THREADS=250
            elif [ "$LINES" -gt 150 ]; then
              JOBS=5; # THREADS=200
            else
              # 1500 < LINES ≤ 15000 → split count up to ceil(LINES/2000), cap at 10
              SUBCOUNT=$(( (LINES + 50 - 1) / 50 ))
              if [ "$SUBCOUNT" -lt 10 ]; then
                JOBS=$SUBCOUNT
              else
                JOBS=8
              fi
             # THREADS=100
            fi

            echo "— Launching parallel -j$JOBS with $THREADS threads"
            #parallel -j"$JOBS" \
           #   httpx -l {} -silent -threads "$THREADS" -retries 1 -random-agent -timeout 10 \
             #       -o "$OUT/httpx_{/.}.txt" \
              #::: "$TMP"/sub_*.txt
            parallel -j"$JOBS" \
              nuclei -l {} -silent -c 10 -probe-concurrency 100 -t ~/nuclei-templates/http/exposed-panels/ -bs 10 -scan-strategy template-spray -rl 50 -o "OUT/httpx_{/.}.txt" ::: "$TMP"/sub_*.txt

            # cleanup temp sub-chunks
            rm -rf "$TMP"
          fi


          # — now RECOMBINE all sub-results into one per original chunk:

          FINAL_TARGET_FILE="${OUT}/httpx_$(basename "$CH")"
          TEMP_AGGREGATED_FILE=$(mktemp "${OUT}/aggregated_XXXXXX.tmp") # Create temp file IN $OUT

          echo "Consolidating results. Final target: $FINAL_TARGET_FILE"
         
          # Collect all source httpx data (httpx_chunk_XX.txt for small, or httpx_sub_YY.txt for large)
          # The -not -name part is just a safeguard for mktemp if it somehow reused a name already matching.
          
          find "$OUT" -maxdepth 1 -type f -name 'httpx_*.txt' -not -name "$(basename "$TEMP_AGGREGATED_FILE")" -print0 \
            | xargs -0 --no-run-if-empty cat \
            | sort -u > "$TEMP_AGGREGATED_FILE"
          
          # Now, atomically replace or create the FINAL_TARGET_FILE with the sorted content
          mv "$TEMP_AGGREGATED_FILE" "$FINAL_TARGET_FILE"
          
          echo "Recombination complete. Final file $FINAL_TARGET_FILE created/updated."
        
          
          # This is only relevant if it was a large file run. It won't harm if it was a small file run (no sub files).
          # Only delete 'httpx_sub_*.txt' and NOT the 'httpx_chunk_*.txt' which is our final file.
          
          echo "Cleaning up intermediate httpx_sub_*.txt files from $OUT..."
          find "$OUT" -maxdepth 1 -type f -name 'httpx_sub_*.txt' -delete
          echo "Cleanup of sub-files done."         
          echo "--- FINAL CHECK BEFORE PROBE STEP ENDS for domain ${{ matrix.pair.domain }} chunk $(basename "$CH") ---"
          EXPECTED_UPLOAD_FILE_PATH="results/${{ matrix.pair.domain }}/httpx_out/httpx_$(basename "$CH")"
          echo "Upload step will look for: $EXPECTED_UPLOAD_FILE_PATH"
          
          echo "Listing contents of $OUT (which is results/${{ matrix.pair.domain }}/httpx_out/):"
          
          echo "Checking specifically for $EXPECTED_UPLOAD_FILE_PATH:"
          if [ -f "$EXPECTED_UPLOAD_FILE_PATH" ]; then
            echo "SUCCESS: File $EXPECTED_UPLOAD_FILE_PATH FOUND."
            ls -l "$EXPECTED_UPLOAD_FILE_PATH"
            echo "Contents (first 3 lines):"
            head -n 3 "$EXPECTED_UPLOAD_FILE_PATH"
          else
            echo "::error::FAILURE: File $EXPECTED_UPLOAD_FILE_PATH NOT FOUND at the end of the probe step!"
            echo "This is why the upload step will fail."
          fi
          echo "--- END OF FINAL CHECK ---"          
            
      - name: Compute artifact details
        id: art_details # Changed id for clarity as it now outputs more
        run: |
          DOMAIN="${{ matrix.pair.domain }}"
          CHUNK_FILE_PATH="${{ matrix.pair.chunk }}" # e.g., results/fig.co/chunks/chunk_01.txt
          
          # Get the basename of the original chunk file (e.g., chunk_01.txt)
          CHUNK_BASENAME=$(basename "$CHUNK_FILE_PATH")
          
          # Construct the artifact name
          ARTIFACT_NAME="httpx-${DOMAIN}-${CHUNK_BASENAME}"
          echo "artifact_name=${ARTIFACT_NAME}" >> $GITHUB_OUTPUT
          
          # Construct the exact path of the file to be uploaded
          # This file is created by the 'Probe with httpx' step's recombination logic
          FILE_TO_UPLOAD_PATH="results/${DOMAIN}/httpx_out/httpx_${CHUNK_BASENAME}" # No extra .txt at the end here
          echo "file_to_upload=${FILE_TO_UPLOAD_PATH}" >> $GITHUB_OUTPUT

          echo ">> DEBUG: Artifact Name: '$ARTIFACT_NAME'"
          echo ">> DEBUG: File to Upload: '$FILE_TO_UPLOAD_PATH'"

          
      - name: Upload per‑chunk httpx results
        uses: actions/upload-artifact@v4
        env:
          ART_NAME: ${{ steps.art.outputs.artifact_name }}
        with:
          name: ${{ steps.art_details.outputs.artifact_name }}
          path: ${{ steps.art_details.outputs.file_to_upload }} # Use the computed path
          if-no-files-found: error

  aggregate_results:

    concurrency:
      group: aggregate-results-${{ matrix.domain }}
      cancel-in-progress: false
    needs: [prepare_matrix, httpx]
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/pcoder7/spider-puredns-actions:latest
      credentials:
        username: ${{ github.actor }}
        password: ${{ secrets.GHCR_TOKEN }}    
    strategy:
      matrix:
        domain: ${{ fromJson(needs.prepare_matrix.outputs.unique_domains) }}
    steps:
      - uses: actions/checkout@v3
        with: { fetch-depth: 0 }
   

      - name: Ensure anew is installed
        run: |
          if ! command -v anew >/dev/null; then
            echo "Installing anew…"
            go install github.com/tomnomnom/anew@latest
          else
            echo "anew already installed"
          fi      
      
      - name: Clean results directory before download
        run: |
          echo "Cleaning results directory..."
          if [ -d "results" ]; then
            # The '|| true' ensures the step doesn't fail if 'results' doesn't exist yet (e.g., first run)
            # or if it contains files that rm might have issues with (though less likely here).
            # A more robust way might be to remove and recreate if it exists.
            rm -rf results/* results/.* 2>/dev/null || true 
            echo "Contents of results directory after cleaning:"
            ls -A results || echo "Results directory is empty or does not exist."
          else
            echo "Results directory does not exist, no cleaning needed."
            mkdir -p results # Ensure it exists for the download step
          fi          
      - name: Download all httpx artifacts
        uses: actions/download-artifact@v4
        with:
          path: results
          pattern: httpx-*-chunk_*.txt
        
      - name: Aggregate for ${{ matrix.domain }}
        run: |
          D="${{ matrix.domain }}"
          # make sure domain dir exists (matches chunks and recon)
          mkdir -p "results/$D" 
          
          echo "Listing contents of 'results' directory after download (artifacts are in subdirs):"
                            
          AGGREGATED_CONTENT_TMP=$(mktemp)
          FINAL_RESULT_FILE="results/$D/httpx_result.txt"

          echo "Searching for chunk files for domain $D..."
          echo "Looking in directories matching 'results/httpx-${D}-chunk_*.txt/' for files named 'httpx_chunk_*.txt'"
          
          # Corrected find: searches within artifact directories (e.g., results/httpx-mydomain-chunk_01.txt/)
          # for the actual result files (e.g., httpx_chunk_01.txt).
          # xargs --no-run-if-empty handles cases where find returns no files.
          # The 2>/dev/null on find suppresses errors if a glob pattern doesn't match any directories.
          find results/httpx-${D}-chunk_*.txt/ -type f -name "httpx_chunk_*.txt" -print0 2>/dev/null | xargs -0 --no-run-if-empty cat > "$AGGREGATED_CONTENT_TMP"
          
          if [ ! -s "$AGGREGATED_CONTENT_TMP" ]; then
            echo "::warning::No content found to aggregate for domain $D from chunk files. '$FINAL_RESULT_FILE' will be empty."
            # Create an empty file if no content was found, so upload-artifact doesn't fail on missing path
            # and downstream steps know this domain had no results.
            >"$FINAL_RESULT_FILE"
          else
            echo "Total lines across chunks for $D (before deduplication):"
            wc -l < "$AGGREGATED_CONTENT_TMP"
            
            echo "Deduplicating content for $D into $FINAL_RESULT_FILE"
            sort -u < "$AGGREGATED_CONTENT_TMP" > "$FINAL_RESULT_FILE"
          fi
          
          rm -f "$AGGREGATED_CONTENT_TMP"
          
          echo "Final aggregated file for $D:"

          echo "Line count in final aggregated file for $D:"
          wc -l < "$FINAL_RESULT_FILE"

      - name: Upload aggregated result for ${{ matrix.domain }}
        uses: actions/upload-artifact@v4
        with:
          name: aggregated-result-${{ matrix.domain }} # This matches the pattern expected by the commit job
          path: results/${{ matrix.domain }}/httpx_result.txt
          if-no-files-found: error # Or 'warn' if an empty/missing result file is acceptable sometimes          
  
  commit_to_external_repo:
    name: Commit Domain-Specific Results to External Repo
    needs: aggregate_results # Runs after all domain aggregations are complete
    runs-on: ubuntu-latest
    # Only run if previous stages were successful and there was data to process
    if: ${{ success() && needs.prepare_matrix.outputs.matrix != '[]' }}
    steps:
        
      - name: Download all aggregated domain results
        uses: actions/download-artifact@v4
        with:
          path: temp_aggregated_results # All artifacts will be downloaded here
          # Pattern to download all artifacts starting with "aggregated-result-"
          pattern: aggregated-result-*
          # 'merge-multiple: false' (default) is appropriate here, as each artifact is distinct.

      - name: Checkout external repository (store-recon)
        uses: actions/checkout@v3
        with:
          repository: pushrockzz/store-recon # IMPORTANT: Replace with actual username/org
          token: ${{ secrets.STORE_RECON_CAT }} # PAT with 'repo' scope for store-recon
          path: store-recon-checkout # Checkout into a specific subdirectory
          ref: main # Or your target branch in store-recon

      - name: Process and Push Each Domain's Results Individually
        working-directory: ./store-recon-checkout # Run git commands in the checkout directory
        shell: bash
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          SOURCE_DOWNLOAD_DIR="../temp_aggregated_results" # Relative to store-recon-checkout
          OVERALL_SUCCESS=true
          TARGET_BRANCH="main" # Or your target branch in store-recon, should match checkout 'ref'

          shopt -s nullglob # Prevent loop from running if no matches
          for artifact_folder in ${SOURCE_DOWNLOAD_DIR}/aggregated-result-*; do
            domain_name_from_artifact=$(basename "$artifact_folder" | sed 's/aggregated-result-//')
            # The source file is named httpx_result.txt within its artifact folder
            source_file_path="${artifact_folder}/httpx_result.txt"

            echo "--- Processing domain: ${domain_name_from_artifact} ---"

            # 1. PULL LATEST from store-recon target branch before processing this domain
            echo "Pulling latest changes from store-recon/${TARGET_BRANCH} for ${domain_name_from_artifact}..."
            PULL_RETRY_COUNT=3
            PULL_RETRY_DELAY_SECONDS=10
            DOMAIN_PULL_SUCCESS=false
            for i_pull in $(seq 1 $PULL_RETRY_COUNT); do
              if [ -d ".git/rebase-apply" ] || [ -d ".git/rebase-merge" ]; then
                 echo "Aborting previous rebase before pull attempt ${i_pull}..."
                 git rebase --abort || true 
              fi
              if git pull --rebase --autostash origin ${TARGET_BRANCH}; then
                echo "Successfully pulled latest changes from store-recon/${TARGET_BRANCH} for ${domain_name_from_artifact}."
                DOMAIN_PULL_SUCCESS=true
                break
              fi
              if [ $i_pull -lt $PULL_RETRY_COUNT ]; then
                echo "Pull attempt $i_pull for ${domain_name_from_artifact} failed. Retrying in $PULL_RETRY_DELAY_SECONDS seconds..."
                sleep $PULL_RETRY_DELAY_SECONDS
              else
                echo "::error::Failed to pull from store-recon/${TARGET_BRANCH} for ${domain_name_from_artifact} after $PULL_RETRY_COUNT attempts. Skipping this domain."
                OVERALL_SUCCESS=false
                break 
              fi
            done

            if [ "$DOMAIN_PULL_SUCCESS" = false ]; then
              continue 
            fi

            # 2. COPY the specific domain file to the desired structure
            if [ -f "$source_file_path" ]; then
              # This defines the target directory: results/<domain_name>/
              target_domain_dir="results/${domain_name_from_artifact}"
              # This defines the target file name: results/<domain_name>/httpx_result.txt
              target_file_path="${target_domain_dir}/httpx_result.txt"

              mkdir -p "$target_domain_dir" # Ensure the directory structure results/<domain_name>/ exists
              cp "$source_file_path" "$target_file_path" # Copy the file
              
              # 3. ADD & COMMIT that specific domain file at its new location
              git add "$target_file_path" # Add results/<domain_name>/httpx_result.txt
              if ! git diff --staged --quiet; then
                COMMIT_MSG="Update HTTPX results for ${domain_name_from_artifact}"
                COMMIT_BODY="Workflow Run ID: ${{ github.run_id }}
                Workflow: ${{ github.workflow }}
                Run URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                git commit -m "${COMMIT_MSG}" -m "${COMMIT_BODY}"
                echo "Committed changes for ${domain_name_from_artifact} (path: ${target_file_path})"

                # 4. PUSH this domain's commit(s) (with retry)
                echo "Pushing changes for ${domain_name_from_artifact} to store-recon/${TARGET_BRANCH}..."
                PUSH_RETRY_COUNT=5
                PUSH_RETRY_DELAY_SECONDS=15
                DOMAIN_PUSH_SUCCESS=false
                for i_push in $(seq 1 $PUSH_RETRY_COUNT); do
                  echo "Pre-push pull --rebase for ${domain_name_from_artifact}, attempt ${i_push}..."
                  if [ -d ".git/rebase-apply" ] || [ -d ".git/rebase-merge" ]; then
                     echo "Aborting previous rebase before pre-push pull attempt ${i_push}..."
                     git rebase --abort || true
                  fi
                  if ! git pull --rebase --autostash origin ${TARGET_BRANCH}; then
                    echo "::warning::Pre-push pull --rebase failed for ${domain_name_from_artifact}. Attempting push anyway..."
                  fi

                  if git push origin HEAD:${TARGET_BRANCH}; then 
                    echo "Successfully pushed changes for ${domain_name_from_artifact} to store-recon/${TARGET_BRANCH}."
                    DOMAIN_PUSH_SUCCESS=true
                    break 
                  fi

                  if [ $i_push -lt $PUSH_RETRY_COUNT ]; then
                    echo "Push attempt $i_push for ${domain_name_from_artifact} failed. Retrying in $PUSH_RETRY_DELAY_SECONDS seconds..."
                    sleep $PUSH_RETRY_DELAY_SECONDS
                  else
                    echo "::error::Push for ${domain_name_from_artifact} to store-recon/${TARGET_BRANCH} failed after $PUSH_RETRY_COUNT attempts."
                    OVERALL_SUCCESS=false
                    echo "Resetting local HEAD to origin/${TARGET_BRANCH} to discard unpushed commit for ${domain_name_from_artifact}."
                    git fetch origin ${TARGET_BRANCH}
                    git reset --hard origin/${TARGET_BRANCH}
                    break 
                  fi
                done
              else
                echo "No changes to commit for ${domain_name_from_artifact} (path: ${target_file_path})"
              fi
            else
              echo "::warning::Result file not found at '$source_file_path' for artifact folder '$artifact_folder'. Skipping this domain."
            fi
          done
          shopt -u nullglob

          echo "--- Domain processing complete ---"
          if [ "$OVERALL_SUCCESS" = true ]; then
            echo "All domains processed and pushed successfully (or no changes detected for them)."
            exit 0
          else
            echo "::error::One or more domains encountered issues during processing or push. Check logs above."
            exit 1
          fi
