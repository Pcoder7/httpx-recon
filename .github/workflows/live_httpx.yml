name: Parallel HTTPX Probe

on:
  workflow_dispatch:

permissions:
  contents: write

env:
  
  MAX_PARALLEL: 20         # <— throttle HTTPX jobs

jobs:
  prepare_matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set_matrix.outputs.matrix }}
      unique_domains: ${{ steps.set_matrix.outputs.unique_domains }}
    steps:
      - uses: actions/checkout@v3
        with: 
          fetch-depth: 0 

      - name: Build domain matrix from results/*/all_resolved.txt
        id: set_matrix
        run: |
          # 1. Discover domain folders with all_resolved.txt
          mapfile -t domains < <(
            find results -mindepth 1 -maxdepth 1 -type d \
              -exec test -f "{}/all_resolved.txt" \; -print \
            | sed 's|results/||'
          )
      
          # 2. If no domains are found, emit an empty matrix
          if [ ${#domains[@]} -eq 0 ]; then
            echo "No domains found under results/ — emitting empty matrix."
            echo "matrix<<EOF" >> $GITHUB_OUTPUT
            echo "[]"           >> $GITHUB_OUTPUT
            echo "EOF"          >> $GITHUB_OUTPUT
          else
            pairs=()
      
            for d in "${domains[@]}"; do
              SRC="results/$d/all_resolved.txt"

              # skip if file somehow missing  
              [ -f "$SRC" ] || continue
              
              # ← step A: get the total line count
              LINES=$(wc -l < "$SRC")
              echo "Source $SRC has $LINES lines"
      
              # 3. Create per-domain chunk dir & split into 100-line files
              CHUNK_DIR="results/$d/chunks"
              mkdir -p "$CHUNK_DIR"
              if [ "$LINES" -ge 200000 ]; then
                echo "— ≥200 000 lines: splitting into 50 equal chunks"
                split -n l/50 \
                      --numeric-suffixes=1 --suffix-length=2 --additional-suffix=.txt \
                      "$SRC" "$CHUNK_DIR/chunk_"
              elif [ "$LINES" -gt 1500 ]; then
                echo "— > 1500 lines: splitting into 20 equal chunks"
                split -n l/20 \
                      --numeric-suffixes=1 --suffix-length=2 --additional-suffix=.txt \
                      "$SRC" "$CHUNK_DIR/chunk_"
              else
                echo "— ≤ 1500 lines: splitting into 200-line chunks"
                split -l 200 \
                      --numeric-suffixes=1 --suffix-length=2 --additional-suffix=.txt \
                      "$SRC" "$CHUNK_DIR/chunk_"
              fi          
      
              # 4. Debug: list out what we just created
              echo "Contents of $CHUNK_DIR:"
              ls -R "$CHUNK_DIR" || echo "(none found)"
      
              # 5. Build a JSON pair for each chunk file
              for CH in "$CHUNK_DIR"/chunk_*; do
                pairs+=( "{\"domain\":\"$d\",\"chunk\":\"$CH\"}" )
              done
            done
      
            # 6. Emit the chunk-level matrix
            echo "matrix<<EOF" >> $GITHUB_OUTPUT
            printf '%s\n' "${pairs[@]}" | jq -s . >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
          fi
      
          # 7. Emit the unique domain list (unchanged)
          domains_json=$(printf '%s\n' "${domains[@]}" | jq -R . | jq -s .)
          {
            echo "unique_domains<<EOF"
            echo "${domains_json}"
            echo "EOF"
          } >> "$GITHUB_OUTPUT"    
                

      - name: Upload chunks artifact
        uses: actions/upload-artifact@v4
        with:
          name: httpx-chunks
          path: results/*/chunks/**
  
  httpx:
    needs: prepare_matrix
    runs-on: ubuntu-latest
    if: ${{ needs.prepare_matrix.outputs.matrix != '[]' }}
    strategy:
      fail-fast: false
      matrix:
        pair: ${{ fromJson(needs.prepare_matrix.outputs.matrix) }}
      max-parallel: 20

    steps:

      - name: Install GNU Parallel
        run: |
          sudo apt-get update
          sudo apt-get install -y parallel    
    
      - name: Setup Go (for go install)
        uses: actions/setup-go@v5
        with:
          go-version: '1.22'
          
      - uses: actions/checkout@v3
      - name: Create mod cache directory
        run: mkdir -p $HOME/go/pkg/mod
      
      - name: Cache Go binaries (httpx, anew)
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
             ~/.cache/go-build
             $HOME/go/bin
          key: go-cache-${{ github.ref_name }} 
          restore-keys: |
            go-cache-
      
      - name: Ensure httpx & anew are installed
        run: |
         
          if ! command -v httpx >/dev/null; then
            go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest
            echo "httpx installed"
            find $HOME -name httpx -type f
            echo "find actually found httpx path"
            
          else
            echo "httpx already cached"
          fi
          if ! command -v anew >/dev/null; then
            go install -v github.com/tomnomnom/anew@latest
            
          else
            echo "anew already cached"
          fi
          ls -R "results"
      
      - name: Download chunk files
        uses: actions/download-artifact@v4
        with: 
          name: httpx-chunks 
          path: results/
      
      #- name: List chunk files
       # run: |
        #  D="${{ matrix.pair.domain }}"
         # echo "Contents of results/${D}/chunks:"
          #ls -R "results/${D}/chunks"
      
      - name: Probe with httpx on ${{ matrix.pair.domain }}
        run: |
       
          D="${{ matrix.pair.domain }}"
          CH="${{ matrix.pair.chunk }}"
          OUT="results/${D}/httpx_out"
          mkdir -p "$OUT"
      
          LINES=$(wc -l < "$CH")
          echo "▶ Processing $CH — $LINES lines"

          if [ "$LINES" -le 1500 ]; then
            # small: single httpx run
            echo "— ≤1500 lines: single httpx with 100 threads"
            httpx -l "$CH" -silent -threads 100 \
              -o "${OUT}/httpx_$(basename "$CH").txt"

          else
            # big: split into 1500-line sub-files
            TMP=$(mktemp -d)
            # suffix-length=2 covers up to 99 subchunks (we’ll have ≤30)
            split -l 1500 --numeric-suffixes=1 --suffix-length=2 \
                  --additional-suffix=.txt \
                  "$CH" "$TMP/sub_"

            # decide parallelism & threads based on original chunk size
            if [ "$LINES" -gt 35000 ]; then
              JOBS=15; THREADS=250
            elif [ "$LINES" -gt 15000 ]; then
              JOBS=12; THREADS=200
            else
              # 1500 < LINES ≤ 15000 → split count up to ceil(LINES/2000), cap at 10
              SUBCOUNT=$(( (LINES + 1500 - 1) / 1500 ))
              if [ "$SUBCOUNT" -lt 10 ]; then
                JOBS=$SUBCOUNT
              else
                JOBS=8
              fi
              THREADS=100
            fi

            echo "— Launching parallel -j$JOBS with $THREADS threads"
            parallel -j"$JOBS" \
              httpx -l {} -silent -threads "$THREADS" -retries 1 -random-agent -timeout 10 \
                    -o "$OUT/httpx_{/.}.txt" \
              ::: "$TMP"/sub_*.txt

            # cleanup temp sub-chunks
            rm -rf "$TMP"
          fi


          # — now RECOMBINE all sub-results into one per original chunk:

          find "$OUT" -type f -name 'httpx_*.txt' -print0 \
            | xargs -0 cat \
            | sort -u > "${OUT}/httpx_$(basename "$CH").txt"
          
            
      - name: Compute per-chunk artifact name
        id: art
        run: |
          D="${{ matrix.pair.domain }}"
          CHN="$(basename "${{ matrix.pair.chunk }}")"
          # this writes an output called 'artifact_name'
          echo "artifact_name=httpx-${D}-${CHN}" >> $GITHUB_OUTPUT
          ANAME="httpx-${D}-${CHN}"
           # debug output so you can see it in the log
          echo ">> DEBUG: artifact_name is '$ANAME'"
            
          
      - name: Upload per‑chunk httpx results
        uses: actions/upload-artifact@v4
        env:
          ART_NAME: ${{ steps.art.outputs.artifact_name }}
        with:
          name: ${{ env.ART_NAME }}
          path: results/${{ matrix.pair.domain }}/httpx_out/*.txt


  aggregate_results:

    concurrency:
      group: aggregate-results-${{ matrix.domain }}
      cancel-in-progress: false
    needs: [prepare_matrix, httpx]
    runs-on: ubuntu-latest
    strategy:
      matrix:
        domain: ${{ fromJson(needs.prepare_matrix.outputs.unique_domains) }}
    steps:
      - uses: actions/checkout@v3
        with: { fetch-depth: 0 }

      - name: Cache Go modules & binaries
        uses: actions/cache@v3
        with:
          path: |
            $HOME/go/pkg/mod
            ~/.cache/go-build
            $HOME/go/bin
          key: go-cache-${{ github.ref_name }}
          restore-keys: |
            go-cache-
      - name: Setup Go (for go install)
        uses: actions/setup-go@v5
        with:
          go-version: '1.22'       

      - name: Ensure anew is installed
        run: |
          if ! command -v anew >/dev/null; then
            echo "Installing anew…"
            go install github.com/tomnomnom/anew@latest
          else
            echo "anew already cached"
          fi          
      - name: Download all httpx artifacts
        uses: actions/download-artifact@v4
        with:
          path: results

      - name: Aggregate for ${{ matrix.domain }}
        run: |
          D="${{ matrix.domain }}"
          # make sure domain dir exists (matches chunks and recon)
          mkdir -p "results/$D" 
          
          echo "Listing contents of 'results' directory after download (artifacts are in subdirs):"
          ls -AlR results # More detailed listing for debugging
                            
          AGGREGATED_CONTENT_TMP=$(mktemp)
          FINAL_RESULT_FILE="results/$D/httpx_result.txt"

          echo "Searching for chunk files for domain $D..."
          echo "Looking in directories matching 'results/httpx-${D}-chunk_*.txt/' for files named 'httpx_chunk_*.txt'"
          
          # Corrected find: searches within artifact directories (e.g., results/httpx-mydomain-chunk_01.txt/)
          # for the actual result files (e.g., httpx_chunk_01.txt).
          # xargs --no-run-if-empty handles cases where find returns no files.
          # The 2>/dev/null on find suppresses errors if a glob pattern doesn't match any directories.
          find results/httpx-${D}-chunk_*.txt/ -type f -name "httpx_chunk_*.txt" -print0 2>/dev/null | xargs -0 --no-run-if-empty cat > "$AGGREGATED_CONTENT_TMP"
          
          if [ ! -s "$AGGREGATED_CONTENT_TMP" ]; then
            echo "::warning::No content found to aggregate for domain $D from chunk files. '$FINAL_RESULT_FILE' will be empty."
            # Create an empty file if no content was found, so upload-artifact doesn't fail on missing path
            # and downstream steps know this domain had no results.
            >"$FINAL_RESULT_FILE"
          else
            echo "Total lines across chunks for $D (before deduplication):"
            wc -l < "$AGGREGATED_CONTENT_TMP"
            
            echo "Deduplicating content for $D into $FINAL_RESULT_FILE"
            sort -u < "$AGGREGATED_CONTENT_TMP" > "$FINAL_RESULT_FILE"
          fi
          
          rm -f "$AGGREGATED_CONTENT_TMP"
          
          echo "Final aggregated file for $D:"
          ls -l "$FINAL_RESULT_FILE"
          echo "Line count in final aggregated file for $D:"
          wc -l < "$FINAL_RESULT_FILE"

      - name: Upload aggregated result for ${{ matrix.domain }}
        uses: actions/upload-artifact@v4
        with:
          name: aggregated-result-${{ matrix.domain }} # This matches the pattern expected by the commit job
          path: results/${{ matrix.domain }}/httpx_result.txt
          if-no-files-found: error # Or 'warn' if an empty/missing result file is acceptable sometimes          
  
  commit_to_external_repo:
    name: Commit Domain-Specific Results to External Repo
    needs: aggregate_results # Runs after all domain aggregations are complete
    runs-on: ubuntu-latest
    # Only run if previous stages were successful and there was data to process
    if: ${{ success() && needs.prepare_matrix.outputs.matrix != '[]' }}
    steps:
      - name: Download all aggregated domain results
        uses: actions/download-artifact@v4
        with:
          path: temp_aggregated_results # All artifacts will be downloaded here
          # Pattern to download all artifacts starting with "aggregated-result-"
          pattern: aggregated-result-*
          # 'merge-multiple: false' (default) is appropriate here, as each artifact is distinct.

      - name: List downloaded artifacts structure for debugging
        run: ls -R temp_aggregated_results

      - name: Checkout external repository (store-recon)
        uses: actions/checkout@v3
        with:
          repository: YOUR_OTHER_ACCOUNT_USERNAME/store-recon # IMPORTANT: Replace with actual username/org
          token: ${{ secrets.STORE_RECON_PAT }} # PAT with 'repo' scope for store-recon
          path: store-recon-checkout # Checkout into a specific subdirectory
          ref: main # Or your target branch in store-recon

      - name: Process and Push Each Domain's Results Individually
        working-directory: ./store-recon-checkout # Run git commands in the checkout directory
        shell: bash
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          SOURCE_DOWNLOAD_DIR="../temp_aggregated_results" # Relative to store-recon-checkout
          OVERALL_SUCCESS=true
          TARGET_BRANCH="main" # Or your target branch in store-recon, should match checkout 'ref'

          shopt -s nullglob # Prevent loop from running if no matches
          for artifact_folder in ${SOURCE_DOWNLOAD_DIR}/aggregated-result-*; do
            domain_name_from_artifact=$(basename "$artifact_folder" | sed 's/aggregated-result-//')
            # The source file is named httpx_result.txt within its artifact folder
            source_file_path="${artifact_folder}/httpx_result.txt"

            echo "--- Processing domain: ${domain_name_from_artifact} ---"

            # 1. PULL LATEST from store-recon target branch before processing this domain
            echo "Pulling latest changes from store-recon/${TARGET_BRANCH} for ${domain_name_from_artifact}..."
            PULL_RETRY_COUNT=3
            PULL_RETRY_DELAY_SECONDS=10
            DOMAIN_PULL_SUCCESS=false
            for i_pull in $(seq 1 $PULL_RETRY_COUNT); do
              if [ -d ".git/rebase-apply" ] || [ -d ".git/rebase-merge" ]; then
                 echo "Aborting previous rebase before pull attempt ${i_pull}..."
                 git rebase --abort || true 
              fi
              if git pull --rebase --autostash origin ${TARGET_BRANCH}; then
                echo "Successfully pulled latest changes from store-recon/${TARGET_BRANCH} for ${domain_name_from_artifact}."
                DOMAIN_PULL_SUCCESS=true
                break
              fi
              if [ $i_pull -lt $PULL_RETRY_COUNT ]; then
                echo "Pull attempt $i_pull for ${domain_name_from_artifact} failed. Retrying in $PULL_RETRY_DELAY_SECONDS seconds..."
                sleep $PULL_RETRY_DELAY_SECONDS
              else
                echo "::error::Failed to pull from store-recon/${TARGET_BRANCH} for ${domain_name_from_artifact} after $PULL_RETRY_COUNT attempts. Skipping this domain."
                OVERALL_SUCCESS=false
                break 
              fi
            done

            if [ "$DOMAIN_PULL_SUCCESS" = false ]; then
              continue 
            fi

            # 2. COPY the specific domain file to the desired structure
            if [ -f "$source_file_path" ]; then
              # This defines the target directory: results/<domain_name>/
              target_domain_dir="results/${domain_name_from_artifact}"
              # This defines the target file name: results/<domain_name>/httpx_result.txt
              target_file_path="${target_domain_dir}/httpx_result.txt"

              mkdir -p "$target_domain_dir" # Ensure the directory structure results/<domain_name>/ exists
              cp "$source_file_path" "$target_file_path" # Copy the file
              
              # 3. ADD & COMMIT that specific domain file at its new location
              git add "$target_file_path" # Add results/<domain_name>/httpx_result.txt
              if ! git diff --staged --quiet; then
                COMMIT_MSG="Update HTTPX results for ${domain_name_from_artifact}"
                COMMIT_BODY="Workflow Run ID: ${{ github.run_id }}
                Workflow: ${{ github.workflow }}
                Run URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                git commit -m "${COMMIT_MSG}" -m "${COMMIT_BODY}"
                echo "Committed changes for ${domain_name_from_artifact} (path: ${target_file_path})"

                # 4. PUSH this domain's commit(s) (with retry)
                echo "Pushing changes for ${domain_name_from_artifact} to store-recon/${TARGET_BRANCH}..."
                PUSH_RETRY_COUNT=5
                PUSH_RETRY_DELAY_SECONDS=15
                DOMAIN_PUSH_SUCCESS=false
                for i_push in $(seq 1 $PUSH_RETRY_COUNT); do
                  echo "Pre-push pull --rebase for ${domain_name_from_artifact}, attempt ${i_push}..."
                  if [ -d ".git/rebase-apply" ] || [ -d ".git/rebase-merge" ]; then
                     echo "Aborting previous rebase before pre-push pull attempt ${i_push}..."
                     git rebase --abort || true
                  fi
                  if ! git pull --rebase --autostash origin ${TARGET_BRANCH}; then
                    echo "::warning::Pre-push pull --rebase failed for ${domain_name_from_artifact}. Attempting push anyway..."
                  fi

                  if git push origin HEAD:${TARGET_BRANCH}; then 
                    echo "Successfully pushed changes for ${domain_name_from_artifact} to store-recon/${TARGET_BRANCH}."
                    DOMAIN_PUSH_SUCCESS=true
                    break 
                  fi

                  if [ $i_push -lt $PUSH_RETRY_COUNT ]; then
                    echo "Push attempt $i_push for ${domain_name_from_artifact} failed. Retrying in $PUSH_RETRY_DELAY_SECONDS seconds..."
                    sleep $PUSH_RETRY_DELAY_SECONDS
                  else
                    echo "::error::Push for ${domain_name_from_artifact} to store-recon/${TARGET_BRANCH} failed after $PUSH_RETRY_COUNT attempts."
                    OVERALL_SUCCESS=false
                    echo "Resetting local HEAD to origin/${TARGET_BRANCH} to discard unpushed commit for ${domain_name_from_artifact}."
                    git fetch origin ${TARGET_BRANCH}
                    git reset --hard origin/${TARGET_BRANCH}
                    break 
                  fi
                done
              else
                echo "No changes to commit for ${domain_name_from_artifact} (path: ${target_file_path})"
              fi
            else
              echo "::warning::Result file not found at '$source_file_path' for artifact folder '$artifact_folder'. Skipping this domain."
            fi
          done
          shopt -u nullglob

          echo "--- Domain processing complete ---"
          if [ "$OVERALL_SUCCESS" = true ]; then
            echo "All domains processed and pushed successfully (or no changes detected for them)."
            exit 0
          else
            echo "::error::One or more domains encountered issues during processing or push. Check logs above."
            exit 1
          fi

delete_artifacts:
    name: Delete Workflow Artifacts
    needs: commit_to_external_repo # Ensure this runs after the commit job
    runs-on: ubuntu-latest
    # 'if: always()' ensures this job runs even if 'commit_to_external_repo' fails,
    # which is generally good for cleanup.
    # If you only want to delete artifacts on successful commit, use 'if: success()'
    if: always()
    permissions:
      actions: write # Required to delete artifacts
    steps:
      - name: Delete all workflow run artifacts
        uses: geekyeggo/delete-artifact@v2 # Using v2 as v4 for delete-artifact doesn't exist and v2 is commonly used
        with:
          # If 'name' is not specified, it defaults to deleting all artifacts for the run.
          # For explicit clarity, you could use:
          # name: |  # A multi-line string to list patterns or names, but we want all
          #  httpx-chunks
          #  httpx-*
          #  aggregated-result-*
          # However, omitting 'name' is simpler for "all artifacts".
          failOnError: false # Set to true if you want the workflow to fail if artifact deletion fails
